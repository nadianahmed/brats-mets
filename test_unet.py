# -*- coding: utf-8 -*-
"""brats-mets_UNet_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-Kn-eE7N1qBWvl8ItyZHc3JBNjoJp6pE

##Google Drive
"""

"""# VS-Code

##Pre-processing

###constants
"""

import os

# Paths
EXTRACTED_FOLDER_NAME = 'ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData'
DATASET_FOLDER = '/project/def-sreeram/hsheikh1/brats-mets/Datasets/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData'  
LABEL_NAME = 'seg'
T1C_SCAN_TYPE = 't1c'
T1N_SCAN_TYPE = 't1n'
T2F_SCAN_TYPE = 't2f'
T2W_SCAN_TYPE = 't2w'
PRE_PROCESSED_IMAGE_SUFFIX = 'processed'
PROJECT_NAME_PREFIX = 'BraTS'

# Image Thresholding
T1C_THRESHOLD_PERCENTILE=0.01 # The percentage of bright pixels to keep.
T1C_SCALE=1.0
T1N_THRESHOLD_PERCENTILE=0.01 # The percentage of bright pixels to keep.
T1N_SCALE=1.0
T2F_THRESHOLD_PERCENTILE=0.01 # The percentage of bright pixels to keep.
T2F_SCALE=1.0
T2W_THRESHOLD_PERCENTILE=0.01 # The percentage of bright pixels to keep.
T2W_SCALE=1.0

# Template Matching
T1C_TUMOUR_SIZE = 10 # Measured in milimeters
T1C_TEMPLATE_MATCH_THRESHOLD = 0.6
T1N_TUMOUR_SIZE = 10 # Measured in milimeters
T1N_TEMPLATE_MATCH_THRESHOLD = 0.6
T2F_TUMOUR_SIZE = 10 # Measured in milimeters
T2F_TEMPLATE_MATCH_THRESHOLD = 0.6
T2W_TUMOUR_SIZE = 10 # Measured in milimeters
T2W_TEMPLATE_MATCH_THRESHOLD = 0.6

APPLY_PRE_PROCESSING = True

"""###data preparation"""

import os
import zipfile
import pandas as pd


def extract_data():
    """
    Loads dataset from already extracted folder (no unzip).

    Returns:
    - pd.DataFrame: a table of the relevant paths.
    """
    extracted_data_folder = '/project/def-sreeram/hsheikh1/brats-mets/Datasets/ASNR-MICCAI-BraTS2023-MET-Challenge-TrainingData'

    if not os.path.isdir(extracted_data_folder):
        raise FileNotFoundError(f"Expected folder not found: {extracted_data_folder} Make sure dataset is extracted.")

    result = pd.DataFrame()

    t1c_scan_paths = []
    t1n_scan_paths = []
    t2f_scan_paths = []
    t2w_scan_paths = []
    preprocessed_paths = []

    label_paths = []
    scan_names = []
    for sample in os.listdir(extracted_data_folder):
        if PROJECT_NAME_PREFIX in sample:
            sample_folder_path = os.path.join(extracted_data_folder, sample)

            scan_names.append(sample)
            t1c_scan_paths.append(os.path.join(sample_folder_path, f"{sample}-{T1C_SCAN_TYPE}.nii.gz"))
            t1n_scan_paths.append(os.path.join(sample_folder_path, f"{sample}-{T1N_SCAN_TYPE}.nii.gz"))
            t2f_scan_paths.append(os.path.join(sample_folder_path, f"{sample}-{T2F_SCAN_TYPE}.nii.gz"))
            t2w_scan_paths.append(os.path.join(sample_folder_path, f"{sample}-{T2W_SCAN_TYPE}.nii.gz"))
            label_paths.append(os.path.join(sample_folder_path, f"{sample}-{LABEL_NAME}.nii.gz"))
            preprocessed_paths.append(os.path.join(sample_folder_path, f"{sample}-{T1C_SCAN_TYPE}-{PRE_PROCESSED_IMAGE_SUFFIX}.nii.gz"))

    result['scan_name'] = scan_names
    result['t1c_path'] = t1c_scan_paths
    result['t1n_path'] = t1n_scan_paths
    result['t2f_path'] = t2f_scan_paths
    result['t2w_path'] = t2w_scan_paths
    result['label_path'] = label_paths

    if APPLY_PRE_PROCESSING:
        result['t1c_processed_scan_path'] = result.apply(lambda row: apply_img_processing(row['t1c_path'], scan_type=T1C_SCAN_TYPE), axis=1)
    else:
        result['t1c_processed_scan_path'] = preprocessed_paths

    return result

"""###image analysis"""

import nibabel as nib
import numpy as np
from skimage.feature import match_template
from scipy.ndimage import gaussian_filter, maximum_filter, label, center_of_mass

def load_image(filename):
    '''
    Loads the image from the given input file.

    Parameters:
    - filename(String): the path to the file.

    Returns:
    - nib.Nifti1Image: the loaded image.
    '''
    img = nib.load(filename)
    return img

def save_image(img, filename, scan_type):
    '''
    Saves the given image into the given filename.

    Parameters:
    - img(Image): the iamge to save.
    - filename(String): the name of the image file to save.
    - scan_type(String): the type of scan.

    Returns:
    - String: the path to the saved image.
    '''
    output_image_name = '{}/{}-{}-{}.nii.gz'.format(get_image_parent_path(path=filename),
                                                    get_image_name_from_path(path=filename),
                                                    scan_type,
                                                    PRE_PROCESSED_IMAGE_SUFFIX)
    nib.save(img, output_image_name)

    return output_image_name

def normalize_image(img):
    '''
    Normalizes the input image using the z-score method.

    Parameters:
    - img(np.ndarray): the input image

    Returns:
    - np.ndarray: the output image after applying the zscore method.
    '''
    mean = np.mean(img)
    std = np.std(img)
    return (img - mean) / std

def apply_threshold_contrast(img, scan_type, show_image=False):
    '''
    Applies thresholding to the given image.

    Parameters:
    - img(Image): the input image.
    - scan_type(String): the type of scan.
    - show_image(Bool): whether the final image should be displayed using matplotlib or not.

    Returns:
    - nib.Nifti1Image: the output image.
    '''
    if scan_type == T1C_SCAN_TYPE:
        threshold_percentile = T1C_THRESHOLD_PERCENTILE
        scale = T1C_SCALE
    elif scan_type == T1N_SCAN_TYPE:
        threshold_percentile = T1N_THRESHOLD_PERCENTILE
        scale = T1N_SCALE
    elif scan_type == T2F_SCAN_TYPE:
        threshold_percentile = T2F_THRESHOLD_PERCENTILE
        scale = T2F_SCALE
    else:
        threshold_percentile = T2W_THRESHOLD_PERCENTILE
        scale = T2W_SCALE

    # Get the contents of the image.
    volume = img.get_fdata()

    # Normalize the image.
    normalized_img = normalize_image(volume)

    # Apply the threshold.
    enhanced = np.copy(volume)
    threshold = np.percentile(normalized_img, (1 - threshold_percentile) * 100)
    enhanced[normalized_img <= threshold] = 0
    enhanced *= scale

    # Clip the output to the correct range.
    enhanced_data = np.clip(enhanced, 0, np.max(enhanced))

    # Generate the output image.
    output_img = nib.Nifti1Image(enhanced_data, img.affine, img.header)

    # Display the output image.
    if show_image:
        ScrollableScanViewer(enhanced_data, title="Thresholded scan of type {}".format(scan_type))

    return output_img

def create_spherical_template(radius):
    '''
    Create a 3D spherecial template with the given radius.

    Parameters:
    - radius(Int): Radius of the sphere in voxels.

    Returns:
    - np.ndarray: 3D binary spherical template.
    '''
    shape = (2 * radius + 1,) * 3
    zz, yy, xx = np.indices(shape)
    center = np.array(shape) // 2
    distance = np.sqrt((zz - center[0])**2 + (yy - center[1])**2 + (xx - center[2])**2)
    sphere = (distance <= radius).astype(np.float32)
    return gaussian_filter(sphere, sigma=1)

def apply_template_matching(img, scan_type, show_image=False):
    '''
    Applies thresholding to the given image.

    Parameters:
    - img(Image): the input image.
    - scan_type(String): the type of scan.
    - show_image(Bool): whether the final image should be displayed using matplotlib or not.

    Returns:
    - nib.Nifti1Image: the output image.
    - [(Int, Int, Int)]: a list of the possible template matches.
    '''
    if scan_type == T1C_SCAN_TYPE:
        radius = T1C_TUMOUR_SIZE
        threshold = T1C_TEMPLATE_MATCH_THRESHOLD
    elif scan_type == T1N_SCAN_TYPE:
        radius = T1N_TUMOUR_SIZE
        threshold = T1N_TEMPLATE_MATCH_THRESHOLD
    elif scan_type == T2F_SCAN_TYPE:
        radius = T2F_TUMOUR_SIZE
        threshold = T2F_TEMPLATE_MATCH_THRESHOLD
    else:
        radius = T2W_TUMOUR_SIZE
        threshold = T2W_TEMPLATE_MATCH_THRESHOLD

    # Get the contents of the image.
    volume = img.get_fdata()

    # Converting milimeters to voxels.
    radius_voxels = round(radius/(img.header.get_zooms()[0]))

    # Generate a spherical template.
    template = create_spherical_template(radius=radius_voxels)
    if template.shape[0] > volume.shape[0] or \
       template.shape[1] > volume.shape[1] or \
       template.shape[2] > volume.shape[2]:
        raise ValueError("Template must be smaller than the volume in all dimensions.")

    # Find correlations to the template.
    correlation = match_template(volume, template, pad_input=True)

    # Find matches based on the correlations.
    local_max = maximum_filter(correlation, size=template.shape) == correlation
    correlation_threshold = threshold * np.max(correlation)
    detected_peaks = (correlation > correlation_threshold) & local_max
    labeled, num_features = label(detected_peaks)
    match_coords_list = center_of_mass(detected_peaks, labeled, range(1, num_features + 1))
    match_coords_list = [tuple(map(int, coords)) for coords in match_coords_list]

    # Display the matches.
    if show_image:
        ScrollableScanViewer(volume, title="Template matched scan of type {}".format(scan_type),
                             match_coords=match_coords_list)

    masked = np.zeros_like(volume)

    # Only keep the matches.
    for x, y, z in match_coords_list:
        # Define bounding box for sphere
        z_min = max(z - radius_voxels, 0)
        z_max = min(z + radius_voxels + 1, volume.shape[0])
        y_min = max(y - radius_voxels, 0)
        y_max = min(y + radius_voxels + 1, volume.shape[1])
        x_min = max(x - radius_voxels, 0)
        x_max = min(x + radius_voxels + 1, volume.shape[2])

        xx, yy, zz = np.ogrid[x_min:x_max, y_min:y_max, z_min:z_max]
        dist = np.sqrt((zz - z)**2 + (yy - y)**2 + (xx - x)**2)
        mask = dist <= radius_voxels

        # Apply spherical mask to copy values
        masked[x_min:x_max, y_min:y_max, z_min:z_max][mask] = \
            volume[x_min:x_max, y_min:y_max, z_min:z_max][mask]

    # Generate the output image.
    output_img = nib.Nifti1Image(masked, img.affine, img.header)

    return output_img, match_coords_list

"""##Helpers

###File Helper
"""

import os

def get_image_parent_path(path):
    '''
    Returns the parent path for the image.

    Parameters:
    - path(String): the path to the image.

    Returns:
    - String: the path to the parent directory of the image.
    '''
    return os.path.dirname(path)

def get_image_name_from_path(path):
    '''
    Extracts the image name from its path.

    Parameters:
    - path(String): the path to the image.

    Returns:
    - String: the name of the image.
    '''
    return os.path.basename(get_image_parent_path(path=path))

"""###Scrollable Scan Viewer"""

import matplotlib.pyplot as plt
from matplotlib.widgets import Slider
import numpy as np

def ScrollableScanViewer(volume, title, match_coords=None, axis=2):
    '''
    View the 3D MRI scan with a slider.

    Optionally, can pass match_coords which is a list of detected tumours.
    '''
    match_coords = match_coords if match_coords is not None else []
    max_index = volume.shape[axis] - 1

    fig, ax = plt.subplots()
    plt.subplots_adjust(bottom=0.2)
    ax.set_title(title)
    ax.axis('off')

    index = max_index // 2

    # Helper to extract slice and overlay points
    def get_slice_and_overlay(idx):
        if axis == 0:
            img = volume[idx, :, :]
            pts = [(y, x) for x, y, z in match_coords if z == idx]
        elif axis == 1:
            img = volume[:, idx, :]
            pts = [(z, x) for x, y, z in match_coords if y == idx]
        else:
            img = volume[:, :, idx]
            pts = [(x, y) for x, y, z in match_coords if z == idx]
        return img.T, pts

    # Initial view
    img, pts = get_slice_and_overlay(index)
    im_display = ax.imshow(img, cmap='gray', origin='lower')
    scatter = ax.plot(*zip(*pts), 'ro')[0] if pts else ax.plot([], [], 'ro')[0]
    coord_text = ax.text(0.02, 0.98, f"Slice: {index}/{max_index}", color='white',
                         transform=ax.transAxes, ha='left', va='top', fontsize=10,
                         bbox=dict(facecolor='black', alpha=0.5))

    # Slider widget
    ax_slider = plt.axes([0.2, 0.05, 0.6, 0.03])
    slider = Slider(ax_slider, f"Slice (axis {axis})", 0, max_index, valinit=index, valstep=1)

    def update_display(idx):
        idx = int(np.clip(idx, 0, max_index))
        img, pts = get_slice_and_overlay(idx)
        im_display.set_data(img)
        if pts:
            xs, ys = zip(*pts)
            scatter.set_data(xs, ys)
        else:
            scatter.set_data([], [])
        coord_text.set_text(f"Slice: {idx}/{max_index}")
        fig.canvas.draw_idle()

    def on_scroll(event):
        step = 1 if event.button == 'up' else -1
        new_idx = int(np.clip(slider.val + step, 0, max_index))
        slider.set_val(new_idx)  # this triggers the update once

    def on_key(event):
        if event.key in ['right', 'left']:
            step = 1 if event.key == 'right' else -1
            new_idx = int(np.clip(slider.val + step, 0, max_index))
            slider.set_val(new_idx)

    slider.on_changed(update_display)
    fig.canvas.mpl_connect('scroll_event', on_scroll)
    fig.canvas.mpl_connect('key_press_event', on_key)

"""##main.py"""

import matplotlib.pyplot as plt

RUN_ALL = False
CHOSEN_TEST_SAMPLE = 'BraTS-MET-00002-000'
SCAN_TYPE = T1C_SCAN_TYPE

def apply_img_processing(filename, scan_type, show_image=False):
    '''
    Applies the correct pre-processing to the image.

    Parameters:
    - filename(String): the filename for the img.
    - scan_type(String): the type of scan.
    - show_image(Bool): whether to display the image or not.

    Returns:
    - String: the path to the resulting image.
    '''
    print(filename)
    img = load_image(filename=filename)
    if show_image:
        ScrollableScanViewer(img.get_fdata(), title="Original scan of type {}".format(scan_type))

    thresholded_img = apply_threshold_contrast(img=img, scan_type=scan_type, show_image=show_image)
    template_matched_image, match_coords_list = apply_template_matching(img=thresholded_img, scan_type=scan_type, show_image=show_image)

    print("Suspected tumour locations for {} of type {}: {}".format(get_image_name_from_path(filename), scan_type,
                                                                    match_coords_list))

    if show_image:
        ScrollableScanViewer(template_matched_image.get_fdata(),
                             title="Final processed scan of type {}".format(scan_type))

    processed_img_path = save_image(template_matched_image, filename=filename, scan_type=scan_type)
    return processed_img_path

# Extracting the datasets
data = extract_data()

# if RUN_ALL:
#     # Apply thresholding to all images.
#     print("Running the T1C scans:")
#     data['t1c_processed_scan_path'] = data.apply(lambda row: apply_img_processing(row['t1c_path'], scan_type=T1C_SCAN_TYPE), axis=1)

#     print("Running the T2F scans:")
#     data['t2f_processed_scan_path'] = data.apply(lambda row: apply_img_processing(row['t2f_path'], scan_type=T2F_SCAN_TYPE), axis=1)

# else:
#     # try a single sample
#     if SCAN_TYPE == T1C_SCAN_TYPE:
#         data[data['scan_name'] == CHOSEN_TEST_SAMPLE].apply(lambda row: apply_img_processing(row['t1c_path'], scan_type=SCAN_TYPE, show_image=True), axis=1)
#     elif SCAN_TYPE == T2F_SCAN_TYPE:
#         data[data['scan_name'] == CHOSEN_TEST_SAMPLE].apply(lambda row: apply_img_processing(row['t2f_path'], scan_type=SCAN_TYPE, show_image=True), axis=1)
#     else:
#         print("DON'T USE THESE SCAN TYPES")

# plt.show()

# Ensure pandas is imported only once
import pandas as pd
import os

# Construct full dataset path

# Compute Canada Environment Update:
# Dataset folder path is now corrected below:

# Construct full dataset path (replace /content/drive/MyDrive/)
dataset_path = os.path.join(DATASET_FOLDER, EXTRACTED_FOLDER_NAME)


# Initialize lists
scan_names = []
t1c_paths = []
t1c_masked_paths = []

# Loop through dataset directory
for sample in os.listdir(dataset_path):
    if PROJECT_NAME_PREFIX in sample:
        sample_folder = os.path.join(dataset_path, sample)

        t1c_file = f"{sample}-{T1C_SCAN_TYPE}.nii.gz"
        t1c_masked_file = f"{sample}-{T1C_SCAN_TYPE}-{PRE_PROCESSED_IMAGE_SUFFIX}.nii.gz"

        t1c_path = os.path.join(sample_folder, t1c_file)
        t1c_masked_path = os.path.join(sample_folder, t1c_masked_file)

        if os.path.isfile(t1c_path) and os.path.isfile(t1c_masked_path):
            scan_names.append(sample)
            t1c_paths.append(t1c_path)
            t1c_masked_paths.append(t1c_masked_path)

# Create DataFrame
df_attention = pd.DataFrame({
    'scan_name': scan_names,
    't1c_path': t1c_paths,
    't1c_masked_path': t1c_masked_paths
})

for sample in os.listdir(dataset_path):
    if PROJECT_NAME_PREFIX in sample:
        sample_folder = os.path.join(dataset_path, sample)

        # Original and processed filenames
        t1c_file = f"{sample}-{T1C_SCAN_TYPE}.nii.gz"
        t1c_masked_file = f"{sample}-{T1C_SCAN_TYPE}-{PRE_PROCESSED_IMAGE_SUFFIX}.nii.gz"

        # Full paths
        t1c_path = os.path.join(sample_folder, t1c_file)
        t1c_masked_path = os.path.join(sample_folder, t1c_masked_file)

        # Check existence of both files before appending
        if os.path.isfile(t1c_path) and os.path.isfile(t1c_masked_path):
            df_attention.loc[len(df_attention)] = {
                'scan_name': sample,
                't1c_path': t1c_path,
                't1c_masked_path': t1c_masked_path
            }

df_attention

"""#UNet Model"""

import torch
import torch.nn as nn

class AttentionBlock3D(nn.Module):
    def __init__(self, F_g, F_l, F_int):
        super(AttentionBlock3D, self).__init__()
        self.W_g = nn.Sequential(
            nn.Conv3d(F_g, F_int, kernel_size=1),
            nn.BatchNorm3d(F_int)
        )
        self.W_x = nn.Sequential(
            nn.Conv3d(F_l, F_int, kernel_size=1),
            nn.BatchNorm3d(F_int)
        )
        self.psi = nn.Sequential(
            nn.Conv3d(F_int + 1, 1, kernel_size=1),  # 1 extra channel for template map
            nn.BatchNorm3d(1),
            nn.Sigmoid()
        )
        self.relu = nn.ReLU(inplace=True)

    def forward(self, g, x, template_map):
        g1 = self.W_g(g)
        x1 = self.W_x(x)
        psi = self.relu(g1 + x1)

        # Resize template_map to match psi shape and concatenate
        if template_map.shape[2:] != psi.shape[2:]:
            template_map = nn.functional.interpolate(template_map, size=psi.shape[2:], mode='trilinear', align_corners=False)
        psi = torch.cat([psi, template_map], dim=1)

        psi = self.psi(psi)
        return x * psi


class AttentionUNet3D(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, features=[32, 64, 128, 256]):
        super(AttentionUNet3D, self).__init__()

        self.encoder1 = self._block(in_channels, features[0])
        self.pool1 = nn.MaxPool3d(2)

        self.encoder2 = self._block(features[0], features[1])
        self.pool2 = nn.MaxPool3d(2)

        self.encoder3 = self._block(features[1], features[2])
        self.pool3 = nn.MaxPool3d(2)

        self.bottleneck = self._block(features[2], features[3])

        self.upconv3 = nn.ConvTranspose3d(features[3], features[2], kernel_size=2, stride=2)
        self.att3 = AttentionBlock3D(F_g=features[2], F_l=features[2], F_int=features[1])
        self.decoder3 = self._block(features[3], features[2])

        self.upconv2 = nn.ConvTranspose3d(features[2], features[1], kernel_size=2, stride=2)
        self.att2 = AttentionBlock3D(F_g=features[1], F_l=features[1], F_int=features[0])
        self.decoder2 = self._block(features[2], features[1])

        self.upconv1 = nn.ConvTranspose3d(features[1], features[0], kernel_size=2, stride=2)
        self.att1 = AttentionBlock3D(F_g=features[0], F_l=features[0], F_int=features[0]//2)
        self.decoder1 = self._block(features[1], features[0])

        self.conv_out = nn.Conv3d(features[0], out_channels, kernel_size=1)

    def forward(self, t1c_input, template_map):
        enc1 = self.encoder1(t1c_input)
        enc2 = self.encoder2(self.pool1(enc1))
        enc3 = self.encoder3(self.pool2(enc2))

        bottleneck = self.bottleneck(self.pool3(enc3))

        dec3 = self.upconv3(bottleneck)
        enc3 = self.att3(g=dec3, x=enc3, template_map=template_map)
        dec3 = self.decoder3(torch.cat((dec3, enc3), dim=1))

        dec2 = self.upconv2(dec3)
        enc2 = self.att2(g=dec2, x=enc2, template_map=template_map)
        dec2 = self.decoder2(torch.cat((dec2, enc2), dim=1))

        dec1 = self.upconv1(dec2)
        enc1 = self.att1(g=dec1, x=enc1, template_map=template_map)
        dec1 = self.decoder1(torch.cat((dec1, enc1), dim=1))

        return self.conv_out(dec1)

    def _block(self, in_channels, out_channels):
        return nn.Sequential(
            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm3d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm3d(out_channels),
            nn.ReLU(inplace=True),
        )

"""##Dataset Model"""

import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import nibabel as nib
import os
import numpy as np

class BRATSMetsDataset(Dataset):
    def __init__(self, dataframe, transform=None):
        """
        Args:
            dataframe (pd.DataFrame): Must contain columns 't1c_path', 't1c_processed_path', 'seg_path'
            transform: Optional transform to be applied on a sample
        """
        self.dataframe = dataframe
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        row = self.dataframe.iloc[idx]

        # Load T1C scan
        t1c_img = nib.load(row['t1c_path']).get_fdata()
        t1c_img = np.expand_dims(t1c_img, axis=0)  # Add channel dimension

        # Load processed scan (attention mask)
        attn_mask = nib.load(row['t1c_processed_scan_path']).get_fdata()
        attn_mask = np.expand_dims(attn_mask, axis=0)

        # Load segmentation label
        seg = nib.load(row['label_path']).get_fdata()
        seg = np.expand_dims(seg, axis=0)

        # Normalize images
        t1c_img = (t1c_img - np.min(t1c_img)) / (np.max(t1c_img) - np.min(t1c_img) + 1e-5)
        attn_mask = (attn_mask - np.min(attn_mask)) / (np.max(attn_mask) - np.min(attn_mask) + 1e-5)

        sample = {
            'image': torch.tensor(t1c_img, dtype=torch.float32),
            'attention': torch.tensor(attn_mask, dtype=torch.float32),
            'label': torch.tensor(seg, dtype=torch.long)
        }

        if self.transform:
            sample = self.transform(sample)

        return sample

def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0.0

    for batch in dataloader:
        inputs = batch['image'].to(device)         # shape: [B, 1, D, H, W]
        attention = batch['attention'].to(device)  # shape: [B, 1, D, H, W]
        labels = batch['label'].to(device)         # shape: [B, 1, D, H, W] or [B, D, H, W]

        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs, attention)         # model expects (input, template_map)
        if outputs.shape != labels.shape:
            labels = labels.squeeze(1)             # adjust if necessary for CrossEntropy

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    print(f"Training Loss: {avg_loss:.4f}")

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Instantiate model
model = AttentionUNet3D(in_channels=1, out_channels=2).to(device)  # Assuming 2-class segmentation

# Optimizer and Loss
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
criterion = torch.nn.CrossEntropyLoss()  # For multi-class segmentation

# Create the dataset
dataset = BRATSMetsDataset(dataframe=data)

# DataLoader
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Train for one epoch
train_one_epoch(model, dataloader, criterion, optimizer, device)
